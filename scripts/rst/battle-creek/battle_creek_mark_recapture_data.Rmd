---
title: "Battle Creek Mark Recapture Data"
author: "Erin Cain"
date: "9/29/2021"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, fig.width=15, fig.height=10)
library(tidyverse)
library(lubridate)
library(googleCloudStorageR)
library(ggplot2)
library(scales)
```

# Battle Creek Mark Recapture Data 

## Description of Monitoring Data

Mike Schraml provided us with Mark Recapture data for Battle Creek. 

**Timeframe:** 2003 - 2021

**Completeness of Record throughout timeframe:** 

**Sampling Location:**
Battle Creek 

**Data Contact:** [Mike Schraml](mailto:mike_schraml@fws.gov)

**Additional description provided by Mike:**

Here are the data you requested. We consider any trial where six or fewer are recaptured to be an invalid trial. Our season average efficiencies are calculated only from valid trial data. During some years we released clipped (upper caudal or lower caudal, or both clip types) and just dyed fish at nearly the same time. These data were combined for the efficiency calculation. See 04/12/12 release data at Vulture Bar (VB) on the Mark-Recap Database MASTER CC DWR Data.xlsx spreadsheet (and below) as an example.

Released
Upper 161
Lower 143
Unclipped 165
Total 469

Recaptured
Upper 2 
Lower 9
Unclipped 10 
Total 21

Bailey's efficiency = (21+1) / (469+/) = 0.0468

In this case, because the fish were released at nearly the same time we would use the upper clip data in the efficiency calculation and consider the trial valid.

I hope this helps you understand these data.  If you have more questions please ask me.



## Access Cloud Data

```{r, eval=FALSE}
# Run Sys.setenv() to specify GCS_AUTH_FILE and GCS_DEFAULT_BUCKET before running 
# getwd() to see how to specify paths 
# Open object from google cloud storage
# Set your authentication using gcs_auth
gcs_auth(json_file = Sys.getenv("GCS_AUTH_FILE"))
# Set global bucket 
gcs_global_bucket(bucket = Sys.getenv("GCS_DEFAULT_BUCKET"))

# git data and save as xlsx
gcs_get_object(object_name = "rst/battle-creek/data-raw/Mark-Recap Database MASTER BC DWR Data.xlsx",
               bucket = gcs_get_global_bucket(),
               saveToDisk = "raw_battle_mark_recapture_data.xlsx",
               overwrite = TRUE)
```

Read in data from google cloud, glimpse raw data: 
Data is stored in a multi-tab sheet, we are interested in tab 2, Data Entry. There are additional non tidy merged cells at the top of the sheet that catagorize variables that we will skip when reading in. 
```{r}
# read in data to clean 
raw_mark_recapture <- readxl::read_excel("raw_battle_mark_recapture_data.xlsx", sheet = 2, skip  = 2) %>% glimpse()
```

## Data transformations

```{r}
# For different excel sheets for each year read in and combine years here
mark_recapture_data <- raw_mark_recapture %>% 
  janitor::clean_names() %>% 
  filter(release_date != "No Mark/Recap Studies for 2014-2015 Season") %>%
  mutate(release_date = janitor::excel_numeric_to_date(as.numeric(as.character(release_date)), date_system = "modern")) %>%
  glimpse

View(raw_mark_recapture)
```


Currently efficiency is just a function of number of number recaptured / number released. (with a x2 adjustment if the trap is fished at 1/2 cone)

## Exploratory Analysis: 

Analysis to explore other variables that may be correlated with trap efficiency: 

```{r}
mark_recapture_data %>% 
  group_by(release_date) %>%
  summarise(daily_flow = mean(flow_release),
            mean_efficency = mean(baileys_trap_efficiency)) %>%
  ggplot() +
  geom_point(aes(x = daily_flow, y = mean_efficency)) + 
  theme_minimal()
```

## Explore Numeric Variables: {.tabset}

```{r}
mark_recapture_data %>% select_if(is.numeric) %>% colnames 
```
The most relevant columns of this dataset are `no_released`, `recaps`, and `baileys_trap_efficiency` 

### Variable: `no_released`

**Plotting no_released over Period of Record**

```{r}
mark_recapture_data %>% ggplot() +
  geom_point(aes(x = release_date, y = no_released))
```

**Numeric Summary of no_released over Period of Record**

```{r}
# Table with summary statistics
summary(mark_recapture_data$no_released)
```

Looks like there are anywhere from 65 - 1143 fish released. 

### Variable: `recaps`

**Plotting recaps over Period of Record**

```{r}
mark_recapture_data %>% ggplot() +
  geom_point(aes(x = release_date, y = recaps))
```

**Numeric Summary of recaps over Period of Record**

```{r}
# Table with summary statistics
summary(mark_recapture_data$recaps)
```

Looks like there are anywhere from 0 - 64 fish recaptured 

### Variable: `baileys_trap_efficiency`

**Plotting baileys_trap_efficiency over Period of Record**

```{r}
mark_recapture_data %>% ggplot() +
  geom_point(aes(x = release_date, y = baileys_trap_efficiency))
```

**Numeric Summary of baileys_trap_efficiency over Period of Record**

```{r}
# Table with summary statistics
summary(mark_recapture_data$baileys_trap_efficiency)
```

Looks like baileys efficiency is anywhere from .22 - 100. 

## Summary of identified issues

* What are all other variables doing in dataset if not being used to calculate trap efficiency? 


TODO decide how we want to use this data and then save a subset of cleaned data to google cloud