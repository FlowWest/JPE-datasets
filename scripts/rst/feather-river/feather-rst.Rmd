---
title: "Feather River RST Catch Data QC"
author: "Erin Cain"
date: "9/29/2021"
output: rmarkdown::github_document
---
  
```{r setup, include=FALSE, fig.width=15, fig.height=10}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(tidyverse)
library(lubridate)
library(googleCloudStorageR)
library(ggplot2)
library(scales)
library(readxl)
```

# Feather River RST Catch Data

## Description of Monitoring Data

Background: The traps are typically operated for approximately seven months (December through June). Two trap locations are necessary because flow is strictly regulated above the Thermalito Outlet and therefore emigration cues and species composition may be different for the two reaches.

**Timeframe:** Dec 1997 - May 2021
  
**Trapping Season:** Typically December - June, looks like it varies quite a bit. 
  
**Completeness of Record throughout timeframe:** Data for every year in the sample timeframe, detailed start and end dates for the season are given in the `survey_year_details` table: 

`
  
**Sampling Location:**
Two RST locations are generally used, one at the lower end of each of the two study reaches. Typically, one RST is stationed at the bottom of Eye Side Channel, RM 60.2 (approximately one mile above the Thermalito Afterbay Outlet) and one stationed in the HFC below Herringer riffle, at RM 45.7.

TODO if time add map with sites here
  
**Data Contact:** 
[Kassie Hickey](mailto:KHickey@psmfc.org)
  
## Access Cloud Data
  
```{r, eval=FALSE}
# Run Sys.setenv() to specify GCS_AUTH_FILE and GCS_DEFAULT_BUCKET before running 
# getwd() to see how to specify paths 
# Open object from google cloud storage
# Set your authentication using gcs_auth
gcs_auth(json_file = Sys.getenv("GCS_AUTH_FILE"))
# Set global bucket 
gcs_global_bucket(bucket = Sys.getenv("GCS_DEFAULT_BUCKET"))

gcs_list_objects()
# git data and save as xlsx
gcs_get_object(object_name = "rst/feather-river/data-raw/Feather River RST Natural Origin Chinook Catch Data_1998-2021.xlsx",
               bucket = gcs_get_global_bucket(),
               saveToDisk = "raw_feather_rst_data.xlsx",
               overwrite = TRUE)

```

Read in data from google cloud, glimpse raw data and domain description sheet: 
```{r}
# read in data to clean
# RST Data
rst_data_sheets <- readxl::excel_sheets("raw_feather_rst_data.xlsx")
survey_year_details  <- readxl::read_excel("raw_feather_rst_data.xlsx", 
                                           sheet = "Survey Year Details") 
survey_year_details
# create function to read in all sheets of a 
read_sheets <- function(sheet){
  data <- read_excel("raw_feather_rst_data.xlsx", sheet = sheet)
}

raw_catch <- purrr::map(rst_data_sheets[-1], read_sheets) %>%
    reduce(bind_rows)

raw_catch %>% glimpse()
```

## Data transformations

```{r}
# Snake case, 
# Columns are appropriate types
# Remove redundant columns
cleaner_catch_data <- raw_catch %>%
  rename("date" = Date, "site_name" = siteName, 
         "at_capture_run" = `At Capture Run`, 
         "lifestage" = lifeStage, 
         "fork_length" = FL, "count" = n) %>%
  mutate(date = as.Date(date)) %>%
  filter(commonName == "Chinook salmon") %>%
  select(-commonName)

cleaner_catch_data %>% glimpse()
```

## Explore Numeric Variables: {.tabset}

```{r}
# Filter clean data to show only numeric variables 
cleaner_catch_data %>% select_if(is.numeric) %>% colnames()
```

### Variable: `fork_length`

**Plotting fork_length**
  
```{r}
cleaner_catch_data %>% filter(fork_length < 250) %>% # filter out 13 points so we can more clearly see distribution
  ggplot(aes(x = fork_length)) + 
  geom_histogram(breaks=seq(0, 200, by=2)) + 
  scale_x_continuous(breaks=seq(0, 200, by=25)) +
  theme_minimal() +
  labs(title = "Fork length distribution") + 
  theme(text = element_text(size = 15),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 
```

```{r}
# TODO put as maybe - think about 
cleaner_catch_data %>% 
  mutate(year = as.factor(year(date))) %>%
  ggplot(aes(x = fork_length, y = year)) + 
  geom_boxplot() + 
  theme_minimal() +
  labs(title = "Fork length summarized by year") + 
  theme(text = element_text(size = 15),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 
```

**Numeric Summary of fork_length over Period of Record**
  
```{r}
# Table with summary statistics
summary(cleaner_catch_data$fork_length)
```

**NA and Unknown Values**
  
* `r round(sum(is.na(cleaner_catch_data$fork_length))/nrow(cleaner_catch_data), 3) * 100` % of values in the `fork_length` column are NA. 

### Variable: `count`

**Plotting passage counts over period of record**
```{r}
# Group by year and create average 

cleaner_catch_data %>% 
  group_by(month = month(date), day = day(date)) %>%
  mutate(average_daily_catch = mean(count)) %>%
  ungroup() %>%
  mutate(year = as.factor(year(date)),
         fake_year = if_else(month(date) %in% 10:12, 1900, 1901),
         fake_date = as.Date(paste0(fake_year,"-", month(date), "-", day(date)))) %>%
  ggplot(aes(x = fake_date, y = average_daily_catch)) + 
  geom_point() + 
  scale_x_date(labels = date_format("%b"), date_breaks = "1 month") + 
  theme_minimal() + 
  theme(text = element_text(size = 15),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = "bottom") + 
  labs(title = "Daily Average Fish Passage Count (Summarized 1997-2021)",
       y = "Average daily catch",
       x = "Date")  

```
  
```{r}
# Figure out a second plot 
cleaner_catch_data  %>%
  mutate(year = as.factor(year(date))) %>%
  ggplot(aes(x = year, y = count)) + 
  geom_col() + 
  theme_minimal() +
  labs(title = "Total Fish Count by Year",
       y = "Total fish count") + 
  theme(text = element_text(size = 15),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 
```

**Numeric Summary of count over Period of Record**
  
```{r}
# Table with summary statistics
summary(cleaner_catch_data$count)
```

**NA and Unknown Values**
  
* `r round(sum(is.na(cleaner_catch_data$count))/nrow(cleaner_catch_data), 3) * 100` % of values in the `count` column are NA. 

## Explore Categorical variables: {.tabset}

```{r}
# Filter clean data to show only categorical variables (this way we know we do not miss any)
cleaner_catch_data %>% select_if(is.character) %>% colnames()
```


### Variable: `site_name`

The feather river efficiency data contains additional information about sub sites and lat/long values. 

```{r}
table(cleaner_catch_data$site_name) 
```

**NA and Unknown Values**
  
* `r round(sum(is.na(cleaner_catch_data$site_name))/nrow(cleaner_catch_data), 3) * 100` % of values in the `site_name` column are NA. 

### Variable: `at_capture_run`
```{r}
table(cleaner_catch_data$at_capture_run) 
cleaner_catch_data$at_capture_run <- ifelse(cleaner_catch_data$at_capture_run == "Not recorded", NA, tolower(cleaner_catch_data$at_capture_run))
```

**NA and Unknown Values**
  
* `r round(sum(is.na(cleaner_catch_data$at_capture_run))/nrow(cleaner_catch_data), 3) * 100` % of values in the `at_capture_run` column are NA. 

### Variable: `lifestage`
```{r}
table(cleaner_catch_data$lifestage) 
cleaner_catch_data$lifestage <- ifelse(cleaner_catch_data$lifestage == "Not recorded", NA, tolower(cleaner_catch_data$lifestage))
```

**NA and Unknown Values**
  
* `r round(sum(is.na(cleaner_catch_data$lifestage))/nrow(cleaner_catch_data), 3) * 100` % of values in the `lifestage` column are NA. 

```{r}
feather_rst <- cleaner_catch_data %>% glimpse()
```

### Save cleaned data back to google cloud 

```{r, eval=FALSE}
# Write to google cloud 
# Name file [watershed]_[data type].csv
f <- function(input, output) write_csv(input, file = output)

gcs_upload(feather_rst,
           object_function = f,
           type = "csv",
           name = "rst/feather-river/data/feather_rst.csv")
```
