---
title: "Feather River RST Catch Data QC"
author: "Erin Cain"
date: "9/29/2021"
output: rmarkdown::github_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, fig.width=15, fig.height=10)
library(tidyverse)
library(lubridate)
library(googleCloudStorageR)
library(ggplot2)
library(scales)
library(readxl)

palette <- wesanderson::wes_palette(name = "Moonrise2")
```

# Feather River RST Catch Data

## Description of Monitoring Data

Background: The traps are typically operated for approximately seven months (December through June). Two trap locations are necessary because flow is strictly regulated above the Thermalito Outlet and therefore emigration cues and species composition may be different for the two reaches.

**QC/raw or estimates** This data is raw passage counts and is QC. 

**Timeframe:** Dec 1997 - May 2021
  
**Trapping Season:** Typically December - June, looks like it varies quite a bit. 
  
**Completeness of Record throughout timeframe:** Data for every year in the sample timeframe, detailed start and end dates for the season are given in the `survey_year_details` table (Only last 5 rows are show please view complete data for more info): 

```{r, echo = FALSE}
survey_year_details  <- readxl::read_excel("raw_feather_rst_data.xlsx", 
                                           sheet = "Survey Year Details") 
knitr::kable(survey_year_details %>% tail(5))
```  

**Sampling Location:**
Two RST locations are generally used, one at the lower end of each of the two study reaches. Typically, one RST is stationed at the bottom of Eye Side Channel, RM 60.2 (approximately one mile above the Thermalito Afterbay Outlet) and one stationed in the HFC below Herringer riffle, at RM 45.7.

See `feather-rst-effort` for additional Location information.     
  
**Data Contact:** 
[Kassie Hickey](mailto:KHickey@psmfc.org)

Questions: all marked or unmarked? 
Units for Fork Length (are these values outrageous)
Try to describe NA's if possible
Fork length by run 
  
## Access Cloud Data
  
```{r, eval=FALSE}
# Run Sys.setenv() to specify GCS_AUTH_FILE and GCS_DEFAULT_BUCKET before running 
# getwd() to see how to specify paths 
# Open object from google cloud storage
# Set your authentication using gcs_auth
gcs_auth(json_file = Sys.getenv("GCS_AUTH_FILE"))
# Set global bucket 
gcs_global_bucket(bucket = Sys.getenv("GCS_DEFAULT_BUCKET"))

gcs_list_objects()
# git data and save as xlsx
gcs_get_object(object_name = "rst/feather-river/data-raw/Feather River RST Natural Origin Chinook Catch Data_1998-2021.xlsx",
               bucket = gcs_get_global_bucket(),
               saveToDisk = "raw_feather_rst_data.xlsx",
               overwrite = TRUE)

```

Read in data from google cloud, glimpse raw data and domain description sheet: 
```{r}
# read in data to clean
# RST Data
rst_data_sheets <- readxl::excel_sheets("raw_feather_rst_data.xlsx")
survey_year_details  <- readxl::read_excel("raw_feather_rst_data.xlsx", 
                                           sheet = "Survey Year Details") 
survey_year_details
# create function to read in all sheets of a 
read_sheets <- function(sheet){
  data <- read_excel("raw_feather_rst_data.xlsx", sheet = sheet)
}

raw_catch <- purrr::map(rst_data_sheets[-1], read_sheets) %>%
    reduce(bind_rows)

raw_catch %>% glimpse()
```

## Data transformations

```{r}
# Snake case, 
# Columns are appropriate types
# Remove redundant columns
cleaner_catch_data <- raw_catch %>%
  rename("date" = Date, "site_name" = siteName, 
         "at_capture_run" = `At Capture Run`, 
         "lifestage" = lifeStage, 
         "fork_length" = FL, "count" = n) %>%
  mutate(date = as.Date(date)) %>%
  filter(commonName == "Chinook salmon") %>%
  select(-commonName)

cleaner_catch_data %>% glimpse()
```

## Explore Numeric Variables: {.tabset}

```{r}
# Filter clean data to show only numeric variables 
cleaner_catch_data %>% select_if(is.numeric) %>% colnames()
```

### Variable: `fork_length`

**Plotting fork_length**
  
```{r}
cleaner_catch_data %>% filter(fork_length < 250) %>% # filter out 13 points so we can more clearly see distribution
  ggplot(aes(x = fork_length)) + 
  geom_histogram(breaks=seq(0, 200, by=2)) + 
  scale_x_continuous(breaks=seq(0, 200, by=25)) +
  theme_minimal() +
  labs(title = "Fork length distribution") + 
  theme(text = element_text(size = 18),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 
```

```{r}
cleaner_catch_data %>% 
  mutate(year = as.factor(year(date))) %>%
  ggplot(aes(x = fork_length, y = year)) + 
  geom_boxplot() + 
  theme_minimal() +
  labs(title = "Fork length summarized by year") + 
  theme(text = element_text(size = 18),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 
```

```{r}
cleaner_catch_data %>% 
  mutate(year = as.factor(year(date))) %>%
  ggplot(aes(x = fork_length, y = lifestage)) + 
  geom_boxplot() + 
  theme_minimal() +
  labs(title = "Fork length summarized by lifestage") + 
  theme(text = element_text(size = 18),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 
```

**Numeric Summary of fork_length over Period of Record**
  
```{r}
# Table with summary statistics
summary(cleaner_catch_data$fork_length)
```

**NA and Unknown Values**
  
* `r round(sum(is.na(cleaner_catch_data$fork_length))/nrow(cleaner_catch_data), 3) * 100` % of values in the `fork_length` column are NA. 

### Variable: `count`

**Plotting raw passage counts over period of record**
```{r, include=FALSE}
sac_indices <- waterYearType::water_year_indices %>% 
    filter(location == "Sacramento Valley") %>% 
    transmute(water_year = WY, year_type = as.character(Yr_type))
```

```{r}
cleaner_catch_data %>% 
  group_by(month = month(date), day = day(date)) %>%
  mutate(average_daily_catch = mean(count)) %>%
  ungroup() %>%
  mutate(year = as.factor(year(date)),
         fake_year = if_else(month(date) %in% 10:12, 1900, 1901),
         fake_date = as.Date(paste0(fake_year,"-", month(date), "-", day(date)))) %>%
  ggplot(aes(x = fake_date, y = average_daily_catch)) + 
  geom_point() + 
  scale_x_date(labels = date_format("%b"), date_breaks = "1 month") + 
  theme_minimal() + 
  theme(text = element_text(size = 18),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = "bottom") + 
  labs(title = "Daily Average Raw Passage Count (Summarized 1997-2021)",
       y = "Average daily catch",
       x = "Date")

```
```{r}
cleaner_catch_data %>% 
  filter(year(date) > 2014, year(date) < 2021) %>%
  mutate(water_year = ifelse(month(date) %in% 10:12, year(date) + 1, year(date))) %>% 
  left_join(sac_indices) %>%
  mutate(year = as.factor(year(date)),
         fake_year = if_else(month(date) %in% 10:12, 1900, 1901),
         fake_date = as.Date(paste0(fake_year,"-", month(date), "-", day(date)))) %>%
  filter(water_year < 2021) %>%
  group_by(date) %>%
  mutate(total_daily_catch = sum(count)) %>%
  ungroup() %>%
  ggplot(aes(x = fake_date, y = total_daily_catch, fill = year_type)) + 
  geom_col() + 
  scale_x_date(labels = date_format("%b"), limits = c(as.Date("1900-10-01"), as.Date("1901-06-01")), date_breaks = "1 month") + 
  theme_minimal() + 
  theme(text = element_text(size = 18),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = "bottom") + 
  labs(title = "Total Daily Raw Passage 2015 - 2020",
       y = "Total daily catch",
       x = "Date")+ 
  facet_wrap(~water_year, scales = "free") + 
  scale_fill_manual(values = palette)
```

  
```{r}
cleaner_catch_data  %>%
  filter(year(date) < 2021) %>%
  mutate(year = as.factor(year(date))) %>%
  ggplot(aes(x = year, y = count)) + 
  geom_col() + 
  theme_minimal() +
  labs(title = "Total Fish Counted each Year by run",
       y = "Total fish count") + 
  theme(text = element_text(size = 18),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  facet_wrap(~at_capture_run, scales = "free")
```

**Numeric Summary of count over Period of Record**
  
```{r}
# Table with summary statistics
summary(cleaner_catch_data$count)
```

**NA and Unknown Values**
  
* `r round(sum(is.na(cleaner_catch_data$count))/nrow(cleaner_catch_data), 3) * 100` % of values in the `count` column are NA. 

## Explore Categorical variables: {.tabset}

```{r}
# Filter clean data to show only categorical variables 
cleaner_catch_data %>% select_if(is.character) %>% colnames()
```


### Variable: `site_name`

The feather river efficiency data contains additional information about sub sites and lat/long values. 

```{r}
table(cleaner_catch_data$site_name) 
```

**NA and Unknown Values**
  
* `r round(sum(is.na(cleaner_catch_data$site_name))/nrow(cleaner_catch_data), 3) * 100` % of values in the `site_name` column are NA. 

### Variable: `at_capture_run`
```{r}
table(cleaner_catch_data$at_capture_run) 
cleaner_catch_data$at_capture_run <- ifelse(cleaner_catch_data$at_capture_run == "Not recorded", NA, tolower(cleaner_catch_data$at_capture_run))
```

**NA and Unknown Values**
  
* `r round(sum(is.na(cleaner_catch_data$at_capture_run))/nrow(cleaner_catch_data), 3) * 100` % of values in the `at_capture_run` column are NA. 

### Variable: `lifestage`
```{r}
table(cleaner_catch_data$lifestage) 
cleaner_catch_data$lifestage <- ifelse(cleaner_catch_data$lifestage == "Not recorded", NA, tolower(cleaner_catch_data$lifestage))
```

**NA and Unknown Values**
  
* `r round(sum(is.na(cleaner_catch_data$lifestage))/nrow(cleaner_catch_data), 3) * 100` % of values in the `lifestage` column are NA. 

```{r}
feather_rst <- cleaner_catch_data %>% glimpse()
```

### Save cleaned data back to google cloud 

```{r, eval=FALSE}
# Write to google cloud 
# Name file [watershed]_[data type].csv
f <- function(input, output) write_csv(input, file = output)

gcs_upload(feather_rst,
           object_function = f,
           type = "csv",
           name = "rst/feather-river/data/feather_rst.csv")
```
